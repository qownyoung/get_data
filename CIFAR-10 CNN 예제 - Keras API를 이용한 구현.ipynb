{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# CIFAR-10 데이터를 다운로드하고 데이터를 불러옵니다.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() # np.array\n",
    "\n",
    "# 이미지들을 float32 데이터 타입으로 변경합니다.\n",
    "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
    "\n",
    "# [0,255] 사이의 값을 [0,1] 사이의 값으로 Normalize 합니다.(세심한 소수점 잊지말기)\n",
    "x_train, x_test = x_train/255., x_test/255.\n",
    "\n",
    "# 스칼라 형태의 레이블(0~9)를 one-hot Encoding 형태로 변환합니다.\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train,10), axis=1) # tf.squeeze는 개수가 1인 dimension을 제거해서 차원 줄여줌\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test,10),axis=1)\n",
    "\n",
    "# y_train_one_hot >> <tf.Tensor 'Squeeze:0' shape=(50000, 10) dtype=float32>\n",
    "# x_train.shape >> (50000, 32, 32, 3)\n",
    "# tf.data API를 이용해서 데이터를 섞고 batch형태로 가져옵니다.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train_one_hot))\n",
    "train_data = train_data.repeat().shuffle(50000).batch(128)\n",
    "# repeat을 사용하면 dataset이 몇 번 반복해서 사용될 지 정할 수 있다. 파라미터가 없다면 계속 반복하고 보통 계속 반복시키고 epoch값을 직접 제어하는 것이 좋다.\n",
    "# shuffle을 사용하면 설정된 epoch마다 dataset을 섞을 수 있다. Dataset의 shuffle은 overfitting을 피할 때 매우 중요\n",
    "train_data_iter = iter(train_data)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test_one_hot))\n",
    "test_data = test_data.batch(1000) # 아까는 train만 배치를 했었는데 이번엔 테스트도\n",
    "test_data_iter = iter(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간에 레이어를 바꿀 수 있는지 궁금\n",
    "# tf.keras.Model을 이용해서 CNN모델을 정의합니다\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 천천히 봐봅시다.\n",
    "        # 첫번째 convolutional layer - 하나의 RGB이미지를 64개의 특징들(feature)로 맵핑(mapping)합니다. padding은 차원을 축소할 것이 아니라면 'same'\n",
    "        self.conv_layer_1 = tf.keras.layers.Conv2D(filters = 64, kernel_size=5, strides=1, padding='same',activation='relu') #2D convolution layer (e.g. spatial convolution over images).\n",
    "        # This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "        self.pool_layer_1 = tf.keras.layers.MaxPool2D(pool_size = (3,3),strides=2)\n",
    "        # max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map. The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling.\n",
    "        \n",
    "        # 두번째 convolutional layer - 64개의 특징들(feature)을 64개의 특징들로 맵핑 -> 왜 64개이지?\n",
    "        self.conv_layer_2 = tf.keras.layers.Conv2D(filters = 64, kernel_size =5, strides = 1, padding='same',activation='relu')\n",
    "        self.pool_layer_2 = tf.keras.layers.MaxPool2D(pool_size = (3,3),strides=2)\n",
    "        # Convolution을 거쳐서 나온 activation maps이 있을 때,이를 이루는 convolution layer을 resizing하여 새로운 layer를 얻는 것 : pooling(sampling)\n",
    "        # 필터랑 커널이 같은거래!\n",
    "        # 아 stride라는 건 그림 전체를 하나의 큰 필터로만 보는 게 아니라, 예를 들어 300*300 사진을 3*3 으로 본다면, 3*3 짜리를 한 칸씩 내려서 볼 건지, 두 칸씩 내릴 건지 필터를 적용한 후의 결과를 feature map 혹은 activation map이라 한다\n",
    "        \n",
    "        # 세번째 convolutional layer\n",
    "        self.conv_layer_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same',activation='relu')\n",
    "        # 네번째 convolutional layer \n",
    "        self.conv_layer_4 = tf.keras.layers.Conv2D(filters=128, kernel_size = 3, strides=1, padding='same',activation='relu')\n",
    "        # 다섯번째 convolutional layer \n",
    "        self.conv_layer_5 = tf.keras.layers.Conv2D(filters=128, kernel_size = 3, strides=1, padding='same',activation='relu')\n",
    "        \n",
    "        # Fully Connected Layer 1: 2 번의 downsampling(샘플링 주기를 낮추는 것) 이후에, 우리의 32*32 이미지는 8*8*128 특징맵(feature map)이 됩니다.\n",
    "        # 이를 384 개 특징들로 맵핑합니다.\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.fc_layer_1 = tf.keras.layers.Dense(384, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2) # 뉴럴 네트워크가 학습중일때, 랜덤하게 뉴런을 꺼서 학습을 방해함으로써, 학습이 학습용 데이타에 치우치는 현상을 막아준다.\n",
    "        \n",
    "        # Fully Connected Layer 2 : 384 개의 특징들(feature)을 10개의 클래스 - airplane, automobile, bird, ,, 로 맵핑합니다\n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation=None)\n",
    "        \n",
    "    def call(self, x, is_training): # 파라미터 넣으면서 아웃풋 반환\n",
    "        #입력 이미지\n",
    "        h_conv1 = self.conv_layer_1(x)\n",
    "        h_pool1 = self.pool_layer_1(h_conv1)\n",
    "        h_conv2 = self.conv_layer_2(h_pool1)\n",
    "        h_pool2 = self.pool_layer-2(h_conv2)\n",
    "        h_conv3 = self.conv_layer_3(h_pool2)\n",
    "        h_conv4 = self.conv_layer_4(h_conv3)\n",
    "        h_conv5 = self.conv_layer_5(h_conv4)\n",
    "        h_conv5_flat = self.flatten_layer(h_conv5)\n",
    "        h_fc1 = self.fc_layer_1(h_conv5_flat)\n",
    "        # Dropout : 모델의 복잡도를 컨트롤, 특징들의 co-adaptation(동조, 비슷한 특징에 집중해버리는)을 방지\n",
    "        h_fc1_drop = self.dropout(h_fc1, training=is_training)\n",
    "        logits = self.output_layer(h_fc1_drop)\n",
    "        y_pred = tf.nn.softmax(logits)\n",
    "        \n",
    "        # 아래에 logit이 필요하므로 logits도 return \n",
    "        # 보통은 트레인에서 드랍아웃을 한다\n",
    "\n",
    "        return y_pred, logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'optimizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-478d16a7ec58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 최적화를 위한 RMSprop 옵티마이저 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 최적화를 위한 function을 정의한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_dw_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accessing local variables before they are created.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\n\u001b[0;32m    108\u001b[0m         name not in self._dw_deprecated_printed):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'optimizers'"
     ]
    }
   ],
   "source": [
    "# cross entropy 손실함수를 정의한다.\n",
    "@tf.function\n",
    "def cross_entropy_loss(logits, y):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# 최적화를 위한 RMSprop 옵티마이저 정의\n",
    "optimizer = tf.optimizers.RMSprop(1e-3)\n",
    "\n",
    "# 최적화를 위한 function을 정의한다.\n",
    "@tf.function\n",
    "def train_step(model,x,y,is_training):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred, logits = model(x, is_training)\n",
    "        loss = cross_entropy_loss(logits,y)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        # loss에 대하여 학습가능한 weight 의 gradient를 알고싶다면 GradientTape scope를 정의해야 합니다. \n",
    "        # optimizer 객체를 사용하여 model.trainable_variables 를 통해 업데이트되는 gradient 를 사용할 수 있습니다.\n",
    "        \n",
    "# 모델의 정확도를 출력하는 함수 정의\n",
    "@tf.function\n",
    "def compute_accuracy(y_pred,y):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = CNN()\n",
    "\n",
    "# 10000step 만큼 최적화 수행\n",
    "for i in range(10000):\n",
    "    batch_x, batch_y = next(train_data_iter)\n",
    "    \n",
    "    # 100step마다 training 데이터 셋에 대한 정확도와 loss를 출력합니다.\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = compute_accuracy(CNN_model(batch_x,False)[0],batch_y)\n",
    "        loss_print = cross_entropy_loss(CNN_model(batch_x, False)[1], batch_y)\n",
    "        \n",
    "        print('반복(Epoch): %d, 트레이닝 데이터 정확도: %f, 손실 함수(loss): %f'%(i,train_accuracy, loss_print))\n",
    "        \n",
    "    # 20% 확률의 dropout 이용해 학습 진행\n",
    "    train_step(CNN_model, batch_x, batch_y, True)\n",
    "    \n",
    "# 학습이 끝나면 테스트 데이터(10000개)에 대한 정확도 출력\n",
    "test_accuracy = 0.0\n",
    "for i in range(10):\n",
    "    test_batch_x, test_batch_y = next(test_data_iter)\n",
    "    test_accuracy = test_accuracy + compute_accuracy(CNN_model(test_batch_x, False)[0], test_batch_y).numpy()\n",
    "test_accuracy = test_accuracy / 10\n",
    "print('테스트 데이터 정확도: %f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#참고\n",
    "#출처: https://bcho.tistory.com/1149 [조대협의 블로그]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
