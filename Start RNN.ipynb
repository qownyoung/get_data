{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN: trained using the current inputs only, not consider previous inputs when generating the current output which means the system didn't have _memory elements._ <br />\n",
    "RNN: use memory(i.e. past inputs to the network) when producing the current output.<br /><br />\n",
    "Many applications involve temporal dependencies(or dependencies over time) which means our current output depends not only on the current input, but also on past inputs.RNN is artificial neural networks that can capture temporal dependencies.<br />\n",
    "* RECURRENT\n",
    "    : Occuring often or repeatedly\n",
    "* RNN\n",
    "    : Perform the same task for each element in the input sequence<br />\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first wave of artificial neural networks in the mid 80s, it became clear that feedforward networks are limited since they are unable to capture temporal dependencies(dependencies that change over time). Modeling temporal data is critical in most real-world applications, since natural signals like speech and video have time varying properties and are characterized by having dependencies across time. By the way, biological neural networks have recurring connections,so applying recurrence to artificial feedforward neural networks made natural sense. The first attempt to add memory to neural networks were the Time Delay Neural Networks(TDNN). And TDNNs, inputs from past timesteps were introduced to the network input, changing the actual external inputs. This had the advantage of clearly allowing the network to look 'beyond' the current timestep, but also introduce to clear disadvantage, since the temporal dependencies were limited to the window of the time chosen. Simple RNNs, also known as Elman networks and Jordan networks were next to follow. It was recognized in the early 90s that all of these networks suffer from what we call, 'vanishing gradient' problem.\n",
    "\n",
    "\n",
    "RNNs have a key flaw, as capturing relationships that span more than 9 or 10 steps back is practically impossible. This flaw stems from the **\"vanishing gradient\"** problem in which the contribution of information decays gemetrically over time. <br />\n",
    "While training network we use **backpropagation**. In the backpropagation process we adjust our weight matrices with the use of a gradient. In the process gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\".<br />\n",
    "**LSTM** is the option to overcome the Vanish Gradient problem in RNNs. Gated Recurrent Units(GRUs) also gives a solution to the problem, by helping us apply networks that have temporal dependencies. <br />\n",
    "The key novelty in LSTMs was the idea that some signals, what we call state variables, can be kept fixed by using gates and reintroduced or not at an appropriate time in the future. In this way, arbitrary time intervals can be represented,and temporal dependencies can be captured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Speech Recognition\n",
    "* Time Series Prediction\n",
    "    + traffic patterns\n",
    "    + movie selection\n",
    "    + stock movement and market conditions\n",
    "* Natural Language Processing\n",
    "    + machine translation\n",
    "    + question answering\n",
    "    + chatbots\n",
    "* Gesture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network - A Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical calculations needed for training RNN systems are fascinating. To deeply understand the process, we first need to feel confident with the **vanilla FFNN system**. We need to thoroughly understand the feedforward process, as well as the backpropagation process used in the training phases of such systems. <br /><br />\n",
    "When working with neural networks we have 2 primary phases:<br /><br />\n",
    "**Training** and **Evaluation**,<br /><br />\n",
    "During the **training** phase, we take the data set (also called the _training set_), which includes many pairs of inputs and their corresponding targets(outputs) -> Supervised Learning Our goal is to find a \"set of weights\" that would best map the inputs to the desired outputs. In the **evaluation** phase, we use the network that was created in the training phase, apply new inputs and expect to obtain the desired output. <br /><br />\n",
    "The training phase will include 2 steps: <br /><br />\n",
    "**Feedfoward** and **Backpropagation** <br /><br />\n",
    "We will repeat these steps as many times as we need until we decide that our system has reached the best set of weights, giving us the best possible outputs.<br /><br />\n",
    "**Goal: To yield a network that generalizes beyond the train set** <br /><br />\n",
    "The output will be compared to the correct output giving us an indication of the error. There are a few ways to determine the error.\n",
    "In the backpropagation part, we will change the weights as we try to minimize the **'error'**, and start the feedforward process all over again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/46597294/72312737-61303900-36cc-11ea-8ecc-2e2c8e3709ed.png)\n",
    "![image](https://user-images.githubusercontent.com/46597294/72312585-e49d5a80-36cb-11ea-84ad-5ea811b60bf9.png)<br />\n",
    "**Activation function** <br />\n",
    "* Allows the network to represent nonlinear relationships between its inputs and its outputs\n",
    "* Most real world data is nonlinear <br />\n",
    "Using these functions can be a bit tricky as they contribute to the vanishing gradient problem.<br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/72312106-5674a480-36ca-11ea-98ba-eeed36f1824a.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72312946-12cf6a00-36cd-11ea-8e41-ad4850cfcee1.png)<br />\n",
    "**Softmax function** <br /><br />\n",
    "We don't need necessarily need an activation function at the output. In some applications, it can be beneficial to use for example, a softmax function, if we want the output values to be between 0 and 1.\n",
    "![image](https://user-images.githubusercontent.com/46597294/72311711-f2051580-36c8-11ea-9138-44f815bed48d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error calculations**<br />\n",
    "For our back propagation calculations, we will use the squared error, which is also called 'loss function'.<br />\n",
    "## Simple Backpropagation Theory \n",
    "Backpropagation is essentially stochastic gradient descent computed using the chain rule. <br /><br />\n",
    "**Simple Weight Update** <br /><br />\n",
    "$ \\underset{new}{W} = \\underset{previous}{W} + {\\alpha}({-{\\partial}E \\over {\\partial}W}) $ <br /><br />\n",
    "* ${\\alpha}$ : learning rate <br />\n",
    "* ${-{\\partial}E \\over {\\partial}W}$ : partial derivative <br />\n",
    "\n",
    "Partial derivative lets us measure how the error is impacted by each weight separately<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "As computing the output, we can finally find the network error. As a reminder, the two Error functions most commonly used are the **Mean Squared Error(MSE)**(usually used in regression problems) and the **cross entropy**(often used in classification problems)<br />\n",
    "For example, let's use a variation of the MSE:<br />\n",
    "$E = {(d-y)^2 \\over 2}$ <br />\n",
    "where $d$ is the desired output and $y$ is the calculated one. Notice that **y** and **d** are not vectors if we have a single output.<br />\n",
    "The error is their squared difference. $E = (d-y)^2$, and is also called the network's **loss function.** We are dividing the error term by 2 to simplify notation. <br />\n",
    "**The aim of the backprop process is to minimize the error, which in our case is the _Loss Function_.** <br /><br />\n",
    "The weight update value $\\bigtriangleup W_{ij}^k$ is calculated with the use of the gradient the following way: <br /><br />\n",
    "$ \\bigtriangleup W_{ij}^k = {\\alpha}({-{\\partial}E \\over {\\partial}W}) $ <br /><br />\n",
    "Therefore: <br /><br />\n",
    "$ \\bigtriangleup W_{ij}^k = \\alpha ({-{\\partial}E \\over {\\partial}W}) = -{\\alpha \\over 2} {{\\partial}(d-y)^2 \\over {\\partial}W_{ij}} = -2{\\alpha \\over 2}(d-y){{\\partial}(d-y) \\over {\\partial}W_{ij}}$ <br /><br />\n",
    "which can be simplified as: <br /><br />\n",
    "$\\bigtriangleup W_{ij}^k = \\alpha(d-y){{\\partial}y \\over {\\partial}W_{ij}}$ <br /><br />\n",
    "(Notice that $d$ is a constant value, so it's partial derivative is simply a zero) This partial derivative of the output with respect to each weight, defines the **gradient** and is often denoted by the Greek letter $\\delta$.<br /><br />\n",
    "$\\delta_{ij} = {{\\partial}y \\over {\\partial}W_{ij}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/46597294/72326672-f6462880-36f2-11ea-8b36-60c41c0fce84.png)\n",
    "![image](https://user-images.githubusercontent.com/46597294/72326746-170e7e00-36f3-11ea-8a28-9f403e059dec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each new input we updated the weights after each calculation of the output. It is often beneficial _**to update the weights once every N steps**_. This is called **mini batch training** and involves averaging the changes to the weights over multiple steps before actually updating the weights.<br />\n",
    "$\\delta = {1 \\over N}\\sum_{h}^{N} \\delta_{ij}$<br />\n",
    "There are two primary reasons for using mini batch training.\n",
    "* Reduction of the complexity of the training process\n",
    "* Noise reduction<br />\n",
    "\n",
    "Noise reduction is that when we average multiple, possibly noisy changes to the weights,we end up with a less noisy correction. This means that the learning process may actually converge faster and more accurately.<br />\n",
    "\n",
    "What's the difference between Batch Gradient Descent and Stochastic Gradient Descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop 에서 error 자체를 MSE로 하네 \n",
    "# GARCH 공부하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "When we minimize the network error using backprop, we may either properly fit the model to the data or overfit. Generally speaking, when we have a finite training set, there's a risk of overfitting.<br />\n",
    "Overfitting means that our model will fit the training data too closely. In other words, we over trained the model or the network to fit out data. As a result, we unintentionally also model the noise or random elements of the training set. If that happens, our model will not generalize well when tested on new inputs. <br />\n",
    "There are generally two main approaches to addressing the overfitting problem.\n",
    "* Stop the training process early\n",
    "* Regularization <br />\n",
    "\n",
    "When we stop the training process early, we do that in the region where the network begins to overfit. By doing so, we reduce degradation in the performance on the test set. It would be ideal if we knew precisely when we should stop the training process, however, that is often difficult to determine. One way of determining when to stop the training is by carving a small dataset out of the training set which we will call the _**validation set**_ Assuming that the accuracy of the validation set is similar to that of the test set, we can use it to estimate when the training should stop. The drawback of this approach is that we end up with fewer samples to train our model on, so our training set is smaller. <br />\n",
    "And alternative mainstream approach to mitigating overfitting is to use _**regularization**_. Regularization means that we impose a constraint on the training of the network such that better generalization can be achieved._**Dropout**_ is a widely used regularization scheme which helps in that manner.\n",
    "<br />\n",
    "## RNN\n",
    "RNNs are based on the same principles as those behind FFNNs, which is why we should remind ourselves of the _feedforward and backpropagation steps that are used in training phase._ <br />\n",
    "There are 2 main differences between FFNNs and RNNs. The Recurrent Neural Network uses: <br />\n",
    "* **sequences** as inputs in the training phase, and \n",
    "* **memory** elements <br />\n",
    "\n",
    "**Memory** is defined as **the output of hidden layer neurons**, which will serve as additional input to the network during next training step.<br /><br />The basic three layer neural network with feedback that serve as memory inputs is called the **Elman Network**(Simple RNN) and is depicted in the following picture:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72397184-72d41800-3782-11ea-9b6a-8cd17cea84dc.png)\n",
    "<br />\n",
    "* Why are these networks called Recurrent Neural Networks?<br >\n",
    "\n",
    "It's simply because with RNN's, we perform the same task for each element in the input sequence.RNN's also attempt to address the need for capturing information in previous inputs by maintaining internal memory elements, also known as States.<br /><br />\n",
    "Many applications have **temporal dependencies**, meaning that, the current output depends not only on the current input, but also on a memory element(feedback) which takes into account past inputs.<br /><br />\n",
    "In FFNN the output at any time $t$ is a function of the current input and the weights. This can be easily expressed using the following equation: <br /><br />\n",
    "$\\bar{y_t} = F(\\bar{x_t},W)$ <br /><br />\n",
    "In RNNs, our output at time $t$, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as: <br /><br />\n",
    "$\\bar{y_t} = F(\\bar{x_t},\\bar{x_{t-1}},\\bar{x_{t-2}},...,\\bar{x_{t_0}},W)$ <br /><br />\n",
    "(There are folded and unfolded model when working with RNNs) \n",
    "![image](https://user-images.githubusercontent.com/46597294/72402269-0cef8c80-3792-11ea-9b44-69c0cd0e9635.png)\n",
    "In FFNNs the hidden layer depended only on the current inputs and weights, as well as on an activation function $\\Phi$ in the following way: <br /><br />$h = \\Phi(\\bar{x}W)$<br />\n",
    "In RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and **also** on the previous state: <br /><br />\n",
    "$\\bar{s_t} = \\Phi(\\bar{x_t}W_x + \\bar{s_{t-1}}W_s)$<br /><br />\n",
    "The output vector is calculated exactly same as in FFNNs.It can be a linear combination of the inputs to each output node with the corresponding weight matrix $W_y$ or a softmax function of the same linear combination.<br /><br />\n",
    "$\\bar{y}_t = \\bar{s}_tW_y$ <br /><br />\n",
    "or<br /><br />\n",
    "$\\bar{y}_t = \\sigma(\\bar{s}_tW_y)$<br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/72398780-78802c80-3787-11ea-8314-f7e82f037a7c.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72398952-078d4480-3788-11ea-844a-dec7459a3f6e.png)\n",
    "\n",
    "## BPTT(Backpropagation Through Time)\n",
    "\n",
    "<br />**Adjusting Weight Matrix $W_s$** <br /><br />\n",
    "${{\\partial}E_N \\over {\\partial}W_S} = \\sum_{i=1}^N {{\\partial}E_N \\over {\\partial}\\bar{y}_N}\\cdot{{\\partial}\\bar{y}_N \\over {\\partial}\\bar{s}_i}\\cdot{{\\partial}\\bar{s}_i \\over {\\partial}W_s}$\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72399956-0873a580-378b-11ea-8207-8e170d97af26.png)\n",
    "\n",
    "<br />**Adjusting Weight Matrix $W_x$** <br /><br />\n",
    "${{\\partial}E_N \\over {\\partial}W_x} = \\sum_{i=1}^N {{\\partial}E_N \\over {\\partial}\\bar{y}_N}\\cdot{{\\partial}\\bar{y}_N \\over {\\partial}\\bar{s}_i}\\cdot{{\\partial}\\bar{s}_i \\over {\\partial}W_x}$\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72400383-5fc64580-378c-11ea-8b14-18437a4ae083.png)\n",
    "\n",
    "### RNN -Reminder\n",
    "$W_x$ - represents the weight matrix connecting the inputs to the state layer.<br />\n",
    "$W_y$ - represents the weight matrix connecting the state to the output.<br />\n",
    "$W_s$ - represents the weight matrix connecting the state from the previous timestep to the state in the following timestep. <br /><br />\n",
    "When training RNNs using BPTT, we can choose to use mini-batches, where we update the weights in batches periodically (as opposed to once every inputs sample). We calculate the gradient for each step but do not update the weights right away. Instead, we update the weights once every fixed number of steps. This helps reduce the complexity of the training process and helps remove noise from the weight updates. <br />\n",
    "The following is the equation used for **Mini-Batch Training Using Gradient Descent**: (where $\\delta_{ij}$ represents the gradient calculated once every inputs sample and M represents the number of gradients we accumulate in the process.) <br /><br />\n",
    "$\\delta_{ij} = {1 \\over M}\\sum_{k=1}^M \\delta_{ij_k}$ <br /><br />\n",
    "If we backpropagate more than ~ 10 timesteps, the gradient will become too small. This phenomena is known as the **vanishinig gradient problem** where the _contribution of information decays geometrically over time._ Therefore temporal dependencies that span many time steps will effectively be discarded by the network. **Long Short-Term Memory (LSTM)** cells were designed to specifically solve this problem.The other scenario to be aware of is that of the **Exploding Gradients**, in which the value of the gradient grows uncontrollably. Luckily, a simple scheme called **Gradient Clipping** practically resolves this issue. Basically what we do is we check in each time step whether the gradient exceeds a certain threshold. <br /><br />\n",
    "$\\delta ={{\\partial}y \\over {\\partial}W_{ij}}$ > Threshold? <br /><br />\n",
    "If it does, we normalize the successive gradient._Normalizing means that we penalize super large gradients more than those that are slightly larger than our threshold._ Clipping large gradients this way helps avoid the Exploding Gradient Problem.\n",
    "\n",
    "## From RNNs To LSTMs\n",
    "* Overcoming the Vanishing Gradient Problem\n",
    "* Inputs can be stored without being forgotten<br />\n",
    "\n",
    "When calculating the gradient using backpropagation through time,the gradients stemming back many timesteps can become negligible. For the same reason, the partial derivative of the error can also become negligible. We want to avoid the loss of information by latching on to some information over many timesteps. Remembering information over long periods of time is easily achieved using these methods.<br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/72403154-84262000-3794-11ea-9b52-4ba644376fc5.png)<br />\n",
    "The main idea with LSTM cells is that they can decide which information to remove or forget,which information to store,and when to use it. The cell can also help decide when to move the previous states information to the next.<br />\n",
    "We just saw (above) that the **LSTM cell has three sigmoids.** Having the data flow through a sigmoid intuitively answers the following questions. Do we let all the data flow through when the output of the sigmoid is one or close to it, or do we force the output to be a zero when none of the data flows through? **These three sigmoids act as a mechanism to filter what goes into the cell if at all, what is retained within the cell, and what passes through to its output.** The key idea in LSTMs is that these three gating functions are also trained using backpropagation by adjusting the weights that feed into them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: Udacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
