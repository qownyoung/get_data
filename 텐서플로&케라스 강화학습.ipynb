{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습과 관련된 수식 속 함축적인 의미에 대해 잘 알지 못하면 강화학습을 직관적으로 이해하기에 한계가 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화의 핵심은 바로 보상을 얻게 해주는 행동의 빈도 증가.<br />\n",
    "강화는 행동심리학에서 연구됐던 분야지만 사실 우리에게 친숙한 개념입니다. 아이가 첫걸음을 떼는 과정에서\n",
    "아이는 지금까지 누구에게 걷는 것을 배운 적이 없지만 스스로 이것저것 시도해 보면서 걷습니다. 그러다 우연히 걷게 되면\n",
    "자신이 하는 행동과 걷게  된다는 보상 사이의 연관관계를 몰라 다시 넘어집니다. 하지만 시간이 지남에 따라 그 관계를 잘 학습해서 결국 잘 걷습니다. \n",
    "이렇게 사람과 동물에게 학습의 기본이 되는 강화라는 개념이 강화학습의 모티프가 된 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 결정을 순차적으로 내려야 하는 문제에 적용됩니다. 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 MDP(Markov Decision Process)입니다. MDP는 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순차적 행동 결정 문제의 구성 요소\n",
    "+ 상태\n",
    "    - _에이전트의 상태, 정적인 요소 뿐만 아니라 에이전트가 움직이는 속도와 같은 동적인 요소도 상태에 포함_\n",
    "+ 행동\n",
    "    + _에이전트가 어떠한 상태에서 취할 수 있는 행동, 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌._\n",
    "+ 보상\n",
    "    + _가장 핵심적인 요소, 강화학습의 목표는 시간에 따라 얻는 보상들의 합을 최대로 하는 정책을 찾는 것, 보상은 에이전트에 속하지 않는 환경의 일부이고 에이전트는 어떤 상황에서 얼마의 보상이 나오는 지 미리 알지 못함._\n",
    "+ 정책\n",
    "    + _제일 좋은 정책은 최적 정책(optimal policy)라고 하며 이 때 보상의 합을 최대로 받을 수 있다. 정책이란 순차적 행동 결정 문제의 처음 부터 끝까지 모든 상태에 대한 에이전트의 행동을 말함._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습의 기초1: MDP와 벨만 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP는 _**상태, 행동, 보상함수, 상태 변환 환률(State Transition Probability), 감가율(Discount Factor)**_ 로 구성돼 있습니다.<br />\n",
    "문제의 정의는 에이전트가 학습하는 데 가장 중요한 단계 중 하나 입니다. 순차적으로 행동을 결정하는 문제에 대한 정의가 바로 MDP입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상태 변환 확률** 은 상태 s에서 행동 a를 취했을 때 다른 상태 s'에 도달할 확률입니다. 이 값은 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부. 상태 변환 확률은 환경의 모델이라고도 부름. 환경은 에이전트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이전트가 갈 상태를 알려줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**감가율**은 시간에 따라서 감가하는 비율을 말하며 $ \\gamma $ 로 표기함. 감가율 $ \\gamma $ 는 0과 1 사이의 값! 따라서 보상에 곱해지면 보상이 감소. 만약 현재의 시간 t로 시간 k가 지난 후에 보상을 $ R_{t+k} $ 를 받을 것이라고 하면 그 보상의 가치는 현재로부터 시간이 k만큼 지났기 때문에 미래에 받을 보상 $ R_{t+k} $ 는 $ \\gamma^{k-1} $ 만큼 감가됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**정책**은 모든 상태에서 에이전트가 할 행동. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수라고 생각가능<br />\n",
    "$ \\pi(a|s) = P[A_t = a|S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의했다. 에이전트는 MDP를 통해 최적 정책을 찾으면 된다. 어떻게 찾을 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반환값의 정의(감가율을 적용한 보상들의 합)<br />\n",
    "$ G_b = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\gamma^3R_{t+4} ... $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가치함수<br />\n",
    "$ v(s) = E[G_t | S_t = s] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞으로 받을 보상에 대한 기댓값인 가치함수 <br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma R_{t+2} +\\gamma^2R_{t+3} ... | S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반환값으로 나타내는 가치함수<br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가치함수로 표현하는 가치함수의 정의<br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma v(S_{t+1})|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책을 고려한 가치함수의 표현 **(벨만 기대 방정식<sup> Bellman Expectation Equation</sup>)** <br />\n",
    "$ v_{\\pi} = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_t = s] $ <br /><br />\n",
    "벨만 기대 방정식은 강화학습에서 상당히 중요. 현재 상태의 가치함수$(v_{\\pi}(s))$와 다음 상태의 가치함수$(v_{\\pi}(s+1))$사이의 관계를 말해주는 방정식. 강화학습은 벨만 방정식을 어떻게 풀어나가느냐의 스토리!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
